<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>FedEx-LoRA: Exact Aggregation for Federated and Efficient Fine-Tuning of Foundation Models</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">FedEx-LoRA: Exact Aggregation for Federated and Efficient Fine-Tuning of Foundation Models</h1>
            <div class="is-size-4 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <span target="_blank">Raghav Singhal<sup>*</sup><sup>1</sup>,</span>
                <span class="author-block">
                <span  target="_blank">Kaustubh Ponkshe<sup>*</sup><sup>1</sup>,</span>
                <span class="author-block">
                <a href="https://praneeth.mit.edu/" target="_blank">Praneeth Vepakomma</a><sup>1,2</sup>
                </span>
                </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1 </sup>Mohamed bin Zayed University of Artificial Intelligence, UAE
                    <br><sup>2 </sup>Massachusetts Institute of Technology, USA</span>
                    <span class="eql-cntrb"><small><br><sup>* </sup>Indicates Equal Contribution</small></span>
                  </div>

                  <div class="is-size-4 publication-authors">
                    <span class="author-block" style="color: red;">A version of our paper is accepted at NeurIPS 2024 Workshop on Fine-Tuning in Modern Machine Learning: Principles and Scalability (FITML)</span>
                    </span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         
                      <!-- Arxiv PDF link -->
                      <!-- <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span> -->

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <!-- <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span> -->

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2410.09432" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
         Your video here -->
<!--         <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section> -->
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Low-Rank Adaptation (LoRA) is a popular technique for efficient fine-tuning of foundation models. However, applying LoRA in 
            federated learning environments, where data is distributed across multiple clients, presents unique challenges. 
            Existing methods rely on traditional federated averaging of LoRA adapters, resulting in inexact updates.
            To address this, we propose Federated Exact LoRA, or FedEx-LoRA, which adds a residual error term to the pretrained frozen weight matrix.
            Our approach achieves exact updates with minimal computational and communication overhead, preserving LoRA’s efficiency. 
            We evaluate the method on various Natural Language Understanding (NLU) and Natural Language Generation (NLG) tasks, showing 
            consistent performance gains over state-of-the-art methods across multiple settings. Through extensive analysis, we quantify 
            that the deviations in updates from the ideal solution are significant, highlighting the need for exact aggregation. 
            Our method’s simplicity, efficiency, and broad applicability position it as a promising solution for accurate and 
            effective federated fine-tuning of foundation models.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Why FedEx-LoRA?</h2>
    <figure>
      <div class="columns is-centered">
        <div class="column">
          <figure class="image">
            <img src="static/images/fig1-left.png" alt="FedIT method">
            <figcaption class="has-text-centered mt-2">
              (a) FedIT (typical federated LoRA approaches)
            </figcaption>
          </figure>
        </div>
        <div class="column">
          <figure class="image">
            <img src="static/images/fig1-right.png" alt="FedEx-LoRA method">
            <figcaption class="has-text-centered mt-2">
              (b) FedEx-LoRA (our proposed method)
            </figcaption>
          </figure>
        </div>
      </div>
      <figcaption class="has-text-justified mt-4">
        Comparison of federated LoRA methods: (a) FedIT averages the individual client low-rank
        adapters <strong>A</strong><sub>i</sub> and <strong>B</strong><sub>i</sub>, resulting in inexact updates. (b) FedEx-LoRA sends the error residual <strong>ΔW</strong><sub>res</sub>
        along with the individual adapters <strong>A</strong><sub>i</sub> and <strong>B</strong><sub>i</sub>, which is added to the pretrained weight matrix <strong>W</strong><sub>0</sub>,
        ensuring exact aggregation. Clients transmit low-rank adapters <strong>A</strong><sub>i</sub> and <strong>B</strong><sub>i</sub> in both methods.
      </figcaption>
    </figure>
  </div>
</section>


<section class="section hero-is-light">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Comparisons with Baselines</h2>

    <!-- Carousel wrapper -->
    <div id="fedex-lora-carousel" class="carousel results-carousel">
      <!-- Carousel item 1 -->
      <div class="item">
        <div class="carousel-content">
          <img src="static/images/results-glue-1.jpg" alt="RoBERTa-base on GLUE">
          <h2 class="subtitle has-text-centered main-caption">
            RoBERTa-base on GLUE
          </h2>
        </div>
      </div>

      <!-- Carousel item 2 -->
      <div class="item">
        <div class="carousel-content">
          <img src="static/images/results-glue-2.jpg" alt="RoBERTa-large on GLUE">
          <h2 class="subtitle has-text-centered main-caption">
            RoBERTa-large on GLUE
          </h2>
        </div>
      </div>

      <!-- Carousel item 3 -->
      <div class="item">
        <div class="carousel-content">
          <img src="static/images/results-nlg.jpg" alt="GPT-2 on E2E-NLG">
          <h2 class="subtitle has-text-centered main-caption">
            GPT-2 on E2E-NLG
          </h2>
        </div>
      </div>
    </div>

    <!-- Caption description below -->
    <div class="content has-text-justified mt-4">
      <p>
        Centralized LoRA (in grey) sets the benchmark skyline for its federated versions. Best results among federated methods (in blue) are
        highlighted in bold for each setting. FedEx-LoRA consistently outperforms existing methods across various tasks and settings.
      </p>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Deviation/Divergence Analysis</h2>

    <!-- Carousel wrapper -->
    <div id="fedex-lora-carousel" class="carousel results-carousel">
      <!-- Carousel item 1 -->
      <div class="item" style="display: flex; flex-direction: column; align-items: center; text-align: center;">
        <img src="static/images/mrpc_r1.png" alt="RoBERTa-base on GLUE" style="width: 100%; max-width: 600px; height: auto; margin-bottom: 10px;">
        <h2 class="subtitle has-text-centered main-caption", style="text-align: left;">
          Scaled Frobenius norm of divergence/deviation of updates with conventional federated
          aggregation (FedAvg) versus ideal LoRA updates, computed after the first aggregation step. We
          plot for query (Q) and value (V) matrices across model layers.
        </h2>
      </div>

      <!-- Carousel item 2 -->
      <div class="item" style="display: flex; flex-direction: column; align-items: center; text-align: center;">
        <img src="static/images/rounds_a0_l10_r1.png" alt="RoBERTa-large on GLUE" style="width: 100%; max-width: 600px; height: auto; margin-bottom: 10px;">
        <h2 class="subtitle has-text-centered main-caption" style="text-align: left;">
          Scaled Frobenius norm of divergence of updates with conventional federated
          aggregation (FedAvg) versus ideal LoRA updates, computed across multiple aggregation rounds for
          various datasets. We present results for the query matrices of the first layer.
        </h2>
      </div>

      <!-- Carousel item 3 -->
      <div class="item" style="display: flex; flex-direction: column; align-items: center; text-align: center;">
        <img src="static/images/rounds_avg_l10_r1.png" alt="GPT-2 on E2E-NLG" style="width: 100%; max-width: 600px; height: auto; margin-bottom: 10px;">
        <h2 class="subtitle has-text-centered main-caption" style="text-align: left;">
          Scaled Frobenius norm of divergence of updates with conventional federated
          aggregation (FedAvg) versus ideal LoRA updates, computed across multiple aggregation rounds for
          various datasets. We present results for the average of the query and value matrices across all layers.
        </h2>
      </div>
    </div>

    <!-- Caption description below -->
    <div class="content has-text-justified mt-4">
      <p>
      We measure the scaled Frobenius norm of the divergence between the updates produced by FedAvg and 
      the ideal LoRA updates, revealing several notable patterns. We observe that the divergence/deviation - (1) decreases as the model
      depth increases, (2) grows with a higher number of local epochs, (3) is more pronounced in the query (Q) matrices compared to the value (V) matrices, 
      (4) consistently decreases as the number of aggregation rounds increases, both for the
      first-layer query matrix and for the average of the query and value matrices across all layers.
      </p>
    </div>
  </div>
</section>





<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
       Paper video. -->
      <!-- <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video"> --> 
            <!-- Youtube embed code here -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->


<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%"> -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
             Your video file here -->
            <!-- <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\ --> -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->






<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{singhal2024exactaggregationfederatedefficient,
        title={Exact Aggregation for Federated and Efficient Fine-Tuning of Foundation Models}, 
        author={Raghav Singhal and Kaustubh Ponkshe and Praneeth Vepakomma},
        year={2024},
        eprint={2410.09432},
        archivePrefix={arXiv},
        primaryClass={cs.DC},
        url={https://arxiv.org/abs/2410.09432}, 
  }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- Default Statcounter code for Fedex-LoRA Project Page
https://raghavsinghal10.github.io/fedex-lora_page/ -->
<script type="text/javascript">
  var sc_project=13045793; 
  var sc_invisible=1; 
  var sc_security="1d08b558"; 
  </script>
  <script type="text/javascript"
  src="https://www.statcounter.com/counter/counter.js" async></script>
  <noscript><div class="statcounter"><a title="Web Analytics"
  href="https://statcounter.com/" target="_blank"><img class="statcounter"
  src="https://c.statcounter.com/13045793/0/1d08b558/1/" alt="Web Analytics"
  referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>
  <!-- End of Statcounter Code -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
